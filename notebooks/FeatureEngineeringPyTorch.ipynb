{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/45113245/how-to-get-mini-batches-in-pytorch-in-a-clean-and-efficient-way\n",
    "# https://distill.pub/2016/misread-tsne/\n",
    "# http://setosa.io/ev/principal-component-analysis/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SESSION = 1\n",
    "SPLIT = 128\n",
    "\n",
    "EPOCHS = 200\n",
    "BATCH_SIZE = 128\n",
    "INPUT_SIZE = 59\n",
    "ENCODE_DIM = 8\n",
    "INSIDE_BATCH = False\n",
    "\n",
    "BASE_FOLDER = Path('../data')\n",
    "SESSIONS = {0: 22, 1: 153, 2: 153}\n",
    "\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaitDataset(Dataset):\n",
    "    def __init__(self, filename):\n",
    "        df = pd.read_csv(BASE_FOLDER.joinpath(Path(filename)), header=None)\n",
    "        y = df[df.columns[-1]].values\n",
    "        df.drop([df.columns[-1]], axis=1, inplace=True)\n",
    "        y = LabelEncoder().fit_transform(y)\n",
    "        \n",
    "        self.Xdata = df\n",
    "        self.Ydata = y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.Xdata)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        vector = self.Xdata.iloc[index, :].values.astype(np.float32)\n",
    "        label  = self.Ydata[index]\n",
    "        \n",
    "        return vector, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_csv(session, split):\n",
    "    return 'zju_gaitaccel_session_' + str(session) + '_' + str(split) + '.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = GaitDataset(get_csv(SESSION, SPLIT))\n",
    "dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "lossloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(INPUT_SIZE, 32),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(16, ENCODE_DIM),\n",
    "            nn.ReLU(True))\n",
    "\n",
    "        self.decoder = nn.Sequential(             \n",
    "            nn.Linear(ENCODE_DIM, 16),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(16, 32),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(32, INPUT_SIZE),\n",
    "            nn.Sigmoid())\n",
    "\n",
    "    def forward(self,x):\n",
    "        x_enc = self.encoder(x)\n",
    "        x_dec = self.decoder(x_enc)\n",
    "        return x_dec, x_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeparatorLoss(nn.Module):\n",
    "    def __init__(self, loader, encoder, writer):\n",
    "        super(SeparatorLoss, self).__init__()\n",
    "        self.loader = loader\n",
    "        self.loader_iter = iter(loader)\n",
    "        self.encoder = encoder\n",
    "        self.pdist = nn.PairwiseDistance(p=2)\n",
    "        self.writer = writer\n",
    "        \n",
    "    def forward(self, x_pred, x_true, encoded, labels, n_iter):    \n",
    "        if INSIDE_BATCH:\n",
    "            # Option A: inside current batch\n",
    "            \n",
    "            # Prepare similarity\n",
    "            sep = torch.pdist(encoded, 2)\n",
    "        else:\n",
    "            # Option B: with other batch\n",
    "        \n",
    "            # Get a batch from the same dataset\n",
    "            try:\n",
    "                batch_X, batch_y = next(self.loader_iter)\n",
    "            except StopIteration:\n",
    "                self.loader_iter = iter(self.loader)\n",
    "                batch_X, batch_y = next(self.loader_iter)\n",
    "                \n",
    "            # Encode it\n",
    "            batch_encoded = self.encoder(batch_X)\n",
    "\n",
    "            # Prepare same class vector\n",
    "            X_labels = labels.view(-1, 1).repeat(1, BATCH_SIZE).view(1, BATCH_SIZE * BATCH_SIZE)\n",
    "            batch_labels = batch_y.view(-1, 1).repeat(1, BATCH_SIZE).view(1, BATCH_SIZE * BATCH_SIZE)\n",
    "            same = (X_labels == batch_labels).to(device, dtype=torch.float32)\n",
    "            same = same * SESSIONS[SESSION] - 1\n",
    "                \n",
    "            # Prepare/repeat matrices\n",
    "            dist_X = encoded.repeat(1, BATCH_SIZE).view(-1, ENCODE_DIM)\n",
    "            dist_batch = batch_encoded.expand(BATCH_SIZE, BATCH_SIZE, ENCODE_DIM).reshape(-1, ENCODE_DIM)\n",
    "\n",
    "            # Calculate pairwise distances\n",
    "            sep = (self.pdist(dist_X, dist_batch) * same).sum() / BATCH_SIZE\n",
    "            writer.add_scalar('data/sep', sep, n_iter)\n",
    " \n",
    "        mse = F.mse_loss(x_pred, x_true)\n",
    "        writer.add_scalar('data/mse', mse, n_iter)\n",
    "#         print('sep: {:.4f}, mse: {:.4f}'.format(sep, mse))\n",
    "        return sep + mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(log_dir='runs/SeparatorTestEmbed')\n",
    "# writer = SummaryWriter()\n",
    "model = Autoencoder().cpu()\n",
    "# distance = nn.MSELoss()\n",
    "distance = SeparatorLoss(lossloader, model.encoder, writer)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), weight_decay=1e-5)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.0003, momentum = 0.9)\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [1/200], loss: 0.2537\n",
      "EMBED\n",
      "epoch [2/200], loss: -4.4228\n",
      "epoch [3/200], loss: -0.4008\n",
      "epoch [4/200], loss: -1.8963\n",
      "epoch [5/200], loss: 4.0500\n",
      "epoch [6/200], loss: 0.1372\n",
      "epoch [7/200], loss: 0.8437\n",
      "epoch [8/200], loss: 5.3576\n",
      "epoch [9/200], loss: 0.4029\n",
      "epoch [10/200], loss: 0.4785\n",
      "epoch [11/200], loss: 3.2824\n",
      "EMBED\n",
      "epoch [12/200], loss: 0.2484\n",
      "epoch [13/200], loss: 0.0125\n",
      "epoch [14/200], loss: 1.1122\n",
      "epoch [15/200], loss: -0.0567\n",
      "epoch [16/200], loss: 0.5823\n",
      "epoch [17/200], loss: 1.0928\n",
      "epoch [18/200], loss: 0.4735\n",
      "epoch [19/200], loss: -0.1486\n",
      "epoch [20/200], loss: 0.3100\n",
      "epoch [21/200], loss: 1.0275\n",
      "EMBED\n",
      "epoch [22/200], loss: 0.4330\n",
      "epoch [23/200], loss: 0.3281\n",
      "epoch [24/200], loss: 0.1735\n",
      "epoch [25/200], loss: 0.3338\n",
      "epoch [26/200], loss: 0.2358\n",
      "epoch [27/200], loss: 0.3983\n",
      "epoch [28/200], loss: 0.5264\n",
      "epoch [29/200], loss: 0.7055\n",
      "epoch [30/200], loss: 0.2485\n",
      "epoch [31/200], loss: 0.4084\n",
      "EMBED\n",
      "epoch [32/200], loss: 0.8920\n",
      "epoch [33/200], loss: 0.6730\n",
      "epoch [34/200], loss: 0.7900\n",
      "epoch [35/200], loss: 0.2305\n",
      "epoch [36/200], loss: 0.2676\n",
      "epoch [37/200], loss: 0.7719\n",
      "epoch [38/200], loss: 0.4174\n",
      "epoch [39/200], loss: 0.4101\n",
      "epoch [40/200], loss: 0.3772\n",
      "epoch [41/200], loss: 0.3151\n",
      "EMBED\n",
      "epoch [42/200], loss: 0.3392\n",
      "epoch [43/200], loss: 0.4038\n",
      "epoch [44/200], loss: 0.2342\n",
      "epoch [45/200], loss: 0.3436\n",
      "epoch [46/200], loss: 0.2091\n",
      "epoch [47/200], loss: 0.1131\n",
      "epoch [48/200], loss: 0.1222\n",
      "epoch [49/200], loss: 0.1919\n",
      "epoch [50/200], loss: 0.3585\n",
      "epoch [51/200], loss: 1.0331\n",
      "EMBED\n",
      "epoch [52/200], loss: 0.1463\n",
      "epoch [53/200], loss: 0.3500\n",
      "epoch [54/200], loss: 0.2192\n",
      "epoch [55/200], loss: 0.4256\n",
      "epoch [56/200], loss: 0.3398\n",
      "epoch [57/200], loss: 0.1896\n",
      "epoch [58/200], loss: 0.2485\n",
      "epoch [59/200], loss: 0.2386\n",
      "epoch [60/200], loss: 0.6706\n",
      "epoch [61/200], loss: 0.2881\n",
      "EMBED\n",
      "epoch [62/200], loss: 0.2795\n",
      "epoch [63/200], loss: 0.3042\n",
      "epoch [64/200], loss: 0.3223\n",
      "epoch [65/200], loss: 0.5062\n",
      "epoch [66/200], loss: 0.5648\n",
      "epoch [67/200], loss: 0.3107\n",
      "epoch [68/200], loss: 0.5371\n",
      "epoch [69/200], loss: 0.4235\n",
      "epoch [70/200], loss: 0.3018\n",
      "epoch [71/200], loss: 0.3129\n",
      "EMBED\n",
      "epoch [72/200], loss: 0.3153\n",
      "epoch [73/200], loss: 0.3011\n",
      "epoch [74/200], loss: 0.3192\n",
      "epoch [75/200], loss: 0.3306\n",
      "epoch [76/200], loss: 0.3011\n",
      "epoch [77/200], loss: 0.3146\n",
      "epoch [78/200], loss: 0.3244\n",
      "epoch [79/200], loss: 0.3219\n",
      "epoch [80/200], loss: 0.2943\n",
      "epoch [81/200], loss: 0.3157\n",
      "EMBED\n",
      "epoch [82/200], loss: 0.2683\n",
      "epoch [83/200], loss: 0.3210\n",
      "epoch [84/200], loss: 0.6421\n",
      "epoch [85/200], loss: 0.3651\n",
      "epoch [86/200], loss: 0.3076\n",
      "epoch [87/200], loss: 0.2995\n",
      "epoch [88/200], loss: 0.3122\n",
      "epoch [89/200], loss: 0.2782\n",
      "epoch [90/200], loss: 0.2876\n",
      "epoch [91/200], loss: 0.3099\n",
      "EMBED\n",
      "epoch [92/200], loss: 0.2349\n",
      "epoch [93/200], loss: 0.7648\n",
      "epoch [94/200], loss: 0.2518\n",
      "epoch [95/200], loss: 0.2953\n",
      "epoch [96/200], loss: 0.9418\n",
      "epoch [97/200], loss: 0.3105\n",
      "epoch [98/200], loss: 0.5520\n",
      "epoch [99/200], loss: 0.5373\n",
      "epoch [100/200], loss: 0.2488\n",
      "epoch [101/200], loss: 0.3037\n",
      "EMBED\n",
      "epoch [102/200], loss: 0.4150\n",
      "epoch [103/200], loss: 0.2539\n",
      "epoch [104/200], loss: 0.1548\n",
      "epoch [105/200], loss: 0.1484\n",
      "epoch [106/200], loss: 0.2854\n",
      "epoch [107/200], loss: -0.1506\n",
      "epoch [108/200], loss: -0.6245\n",
      "epoch [109/200], loss: 0.3212\n",
      "epoch [110/200], loss: 0.9291\n",
      "epoch [111/200], loss: 0.2515\n",
      "EMBED\n",
      "epoch [112/200], loss: 0.0768\n",
      "epoch [113/200], loss: 0.2212\n",
      "epoch [114/200], loss: 0.0271\n",
      "epoch [115/200], loss: -0.6788\n",
      "epoch [116/200], loss: -0.6343\n",
      "epoch [117/200], loss: 0.2811\n",
      "epoch [118/200], loss: 0.0056\n",
      "epoch [119/200], loss: 0.4305\n",
      "epoch [120/200], loss: -3.4354\n",
      "epoch [121/200], loss: 0.3099\n",
      "EMBED\n",
      "epoch [122/200], loss: 0.2113\n",
      "epoch [123/200], loss: 0.3195\n",
      "epoch [124/200], loss: 0.3292\n",
      "epoch [125/200], loss: 0.3453\n",
      "epoch [126/200], loss: 0.1715\n",
      "epoch [127/200], loss: 0.5238\n",
      "epoch [128/200], loss: 0.2874\n",
      "epoch [129/200], loss: 0.2884\n",
      "epoch [130/200], loss: 0.2789\n",
      "epoch [131/200], loss: 0.0589\n",
      "EMBED\n",
      "epoch [132/200], loss: 0.0159\n",
      "epoch [133/200], loss: 0.0762\n",
      "epoch [134/200], loss: -0.1181\n",
      "epoch [135/200], loss: 0.5322\n",
      "epoch [136/200], loss: 0.1677\n",
      "epoch [137/200], loss: 0.6394\n",
      "epoch [138/200], loss: -0.0836\n",
      "epoch [139/200], loss: 0.1418\n",
      "epoch [140/200], loss: 0.0413\n",
      "epoch [141/200], loss: 0.3428\n",
      "EMBED\n",
      "epoch [142/200], loss: 0.1026\n",
      "epoch [143/200], loss: 0.0528\n",
      "epoch [144/200], loss: 0.1426\n",
      "epoch [145/200], loss: 0.1985\n",
      "epoch [146/200], loss: 0.5954\n",
      "epoch [147/200], loss: 0.0145\n",
      "epoch [148/200], loss: 0.1853\n",
      "epoch [149/200], loss: 0.2567\n",
      "epoch [150/200], loss: 0.2425\n",
      "epoch [151/200], loss: 0.2787\n",
      "EMBED\n",
      "epoch [152/200], loss: 0.2892\n",
      "epoch [153/200], loss: 0.2575\n",
      "epoch [154/200], loss: 0.2908\n",
      "epoch [155/200], loss: 0.2910\n",
      "epoch [156/200], loss: 0.2335\n",
      "epoch [157/200], loss: 0.2911\n",
      "epoch [158/200], loss: 0.2398\n",
      "epoch [159/200], loss: 0.2449\n",
      "epoch [160/200], loss: 0.2556\n",
      "epoch [161/200], loss: 0.2084\n",
      "EMBED\n",
      "epoch [162/200], loss: 0.2526\n",
      "epoch [163/200], loss: 0.2416\n",
      "epoch [164/200], loss: 0.2511\n",
      "epoch [165/200], loss: 0.2569\n",
      "epoch [166/200], loss: 0.2479\n",
      "epoch [167/200], loss: 0.2465\n",
      "epoch [168/200], loss: 0.2455\n",
      "epoch [169/200], loss: 0.2201\n",
      "epoch [170/200], loss: 0.2517\n",
      "epoch [171/200], loss: 0.3157\n",
      "EMBED\n",
      "epoch [172/200], loss: 0.2543\n",
      "epoch [173/200], loss: 0.2676\n",
      "epoch [174/200], loss: 0.2389\n",
      "epoch [175/200], loss: 0.2468\n",
      "epoch [176/200], loss: 0.2351\n",
      "epoch [177/200], loss: 0.2489\n",
      "epoch [178/200], loss: 0.1966\n",
      "epoch [179/200], loss: 0.2885\n",
      "epoch [180/200], loss: 0.2343\n",
      "epoch [181/200], loss: 0.1172\n",
      "EMBED\n",
      "epoch [182/200], loss: 0.0440\n",
      "epoch [183/200], loss: 0.1689\n",
      "epoch [184/200], loss: 0.1863\n",
      "epoch [185/200], loss: 0.1548\n",
      "epoch [186/200], loss: 0.1075\n",
      "epoch [187/200], loss: -0.0418\n",
      "epoch [188/200], loss: -0.0967\n",
      "epoch [189/200], loss: 0.2458\n",
      "epoch [190/200], loss: 0.4171\n",
      "epoch [191/200], loss: 0.2259\n",
      "EMBED\n",
      "epoch [192/200], loss: 0.2295\n",
      "epoch [193/200], loss: 0.2674\n",
      "epoch [194/200], loss: 0.2259\n",
      "epoch [195/200], loss: 0.2103\n",
      "epoch [196/200], loss: 0.1993\n",
      "epoch [197/200], loss: 0.2099\n",
      "epoch [198/200], loss: 0.2118\n",
      "epoch [199/200], loss: 0.2192\n",
      "epoch [200/200], loss: 0.2027\n",
      "Wall time: 20min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "i = 0\n",
    "for epoch in range(EPOCHS):\n",
    "    for data in dataloader:\n",
    "        vec, labels = data\n",
    "        vec = Variable(vec, requires_grad=True).cpu()\n",
    "        # ===================forward=====================\n",
    "        dec, enc = model(vec)\n",
    "        loss = distance(dec, vec, enc, labels, i)\n",
    "        # ===================backward====================\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        i += 1\n",
    "    # ===================log========================\n",
    "    print('epoch [{}/{}], loss: {:.4f}'.format(epoch + 1, EPOCHS, loss.item()))\n",
    "    writer.add_scalar('data/loss', loss.item(), epoch + 1)\n",
    "    \n",
    "    # Add embeddings\n",
    "    if epoch % 10 == 0:\n",
    "        encs = torch.Tensor([])\n",
    "        labels = []\n",
    "        for data in dataloader:\n",
    "            vec, label = data\n",
    "            vec = Variable(vec).cpu()\n",
    "            _, enc = model(vec)\n",
    "\n",
    "            encs = torch.cat((encs, enc))\n",
    "            labels.extend([str(l.tolist()) for l in label])\n",
    "            \n",
    "        writer.add_embedding(encs, metadata=labels, global_step=epoch + 1, tag='train')\n",
    "        \n",
    "# writer.export_scalars_to_json('./all_scalars.json')\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
