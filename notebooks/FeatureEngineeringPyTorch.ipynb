{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THEORY\n",
    "# https://distill.pub/2016/misread-tsne/\n",
    "# http://setosa.io/ev/principal-component-analysis/\n",
    "# https://stats.stackexchange.com/questions/245168/choosing-the-hyperparameters-using-t-sne-for-classification/245617\n",
    "# https://nlml.github.io/in-raw-numpy/in-raw-numpy-t-sne/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE\n",
    "# https://medium.com/@vaibhaw.vipul/building-autoencoder-in-pytorch-34052d1d280c\n",
    "# https://stackoverflow.com/questions/45113245/how-to-get-mini-batches-in-pytorch-in-a-clean-and-efficient-way\n",
    "# https://stackoverflow.com/questions/50544730/how-do-i-split-a-custom-dataset-into-training-and-test-datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "SESSION = 1\n",
    "SPLIT = 128\n",
    "RANDOM_SEED = 42\n",
    "SHUFFLE = True\n",
    "\n",
    "TEST_SESSION = 0\n",
    "\n",
    "VALIDATION_SPLIT = 0.2\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 128\n",
    "INPUT_SIZE = 59\n",
    "ENCODE_DIM = 8\n",
    "LOG_EMBED = False\n",
    "\n",
    "BASE_FOLDER = Path('../data')\n",
    "SESSIONS = {0: 22, 1: 153, 2: 153}\n",
    "\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaitDataset(Dataset):\n",
    "    def __init__(self, filename):\n",
    "        df = pd.read_csv(BASE_FOLDER.joinpath(Path(filename)), header=None)\n",
    "        y = df[df.columns[-1]].values\n",
    "        df.drop([df.columns[-1]], axis=1, inplace=True)\n",
    "        y = LabelEncoder().fit_transform(y)\n",
    "        \n",
    "        self.Xdata = df\n",
    "        self.Ydata = y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.Xdata)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        vector = self.Xdata.iloc[index, :].values.astype(np.float32)\n",
    "        label  = self.Ydata[index]\n",
    "        \n",
    "        return vector, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_csv(session, split):\n",
    "    return 'zju_gaitaccel_session_' + str(session) + '_' + str(split) + '.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "gait_dataset = GaitDataset(get_csv(SESSION, SPLIT))\n",
    "\n",
    "# Creating data indices for training and validation splits:\n",
    "dataset_size = len(gait_dataset)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(VALIDATION_SPLIT * dataset_size))\n",
    "if SHUFFLE:\n",
    "    np.random.seed(RANDOM_SEED)\n",
    "    np.random.shuffle(indices)\n",
    "train_indices, valid_indices = indices[split:], indices[:split]\n",
    "\n",
    "# Creating data samplers and loaders:\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(valid_indices)\n",
    "\n",
    "train_loader = DataLoader(gait_dataset, batch_size=BATCH_SIZE, \n",
    "                          sampler=train_sampler)\n",
    "valid_loader = DataLoader(gait_dataset, batch_size=BATCH_SIZE,\n",
    "                          sampler=valid_sampler)\n",
    "lossloader = DataLoader(gait_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "gait_test_dataset = GaitDataset(get_csv(TEST_SESSION, SPLIT))\n",
    "test_loader = DataLoader(gait_test_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "testlossloader = DataLoader(gait_test_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(INPUT_SIZE, 32),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(32, ENCODE_DIM),\n",
    "            nn.ReLU(True))\n",
    "\n",
    "        self.decoder = nn.Sequential(             \n",
    "            nn.Linear(ENCODE_DIM, 32),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(32, INPUT_SIZE))\n",
    "\n",
    "    def forward(self,x):\n",
    "        x_enc = self.encoder(x)\n",
    "        x_dec = self.decoder(x_enc)\n",
    "        return x_dec, x_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = torch.tensor([1, 2, 3])\n",
    "x1_rep = x1.view(-1, 1).repeat(1, 4).view(1, 12)\n",
    "x1_rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2 = torch.tensor([1, 2, 3, 4])\n",
    "x2_rep = x2.repeat(1, 3)\n",
    "x2_rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[24, -1, -1, -1, -1, 24, -1, -1, -1, -1, 24, -1]], dtype=torch.int32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x1_rep == x2_rep).to(device, dtype=torch.int32) * 25 - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeparatorLoss(nn.Module):\n",
    "    def __init__(self, loader, encoder):\n",
    "        super(SeparatorLoss, self).__init__()\n",
    "        self.loader = loader\n",
    "        self.loader_iter = iter(loader)\n",
    "        self.encoder = encoder\n",
    "        self.pdist = nn.PairwiseDistance(p=2)\n",
    "        \n",
    "    def forward(self, x_pred, x_true, encoded, labels, n_iter):    \n",
    "        # Get a batch from the same dataset\n",
    "        try:\n",
    "            batch_X, batch_y = next(self.loader_iter)\n",
    "        except StopIteration:\n",
    "            self.loader_iter = iter(self.loader)\n",
    "            batch_X, batch_y = next(self.loader_iter)\n",
    "\n",
    "        # Encode it\n",
    "        batch_encoded = self.encoder(batch_X)\n",
    "        outer_batch_size = batch_y.shape[0]\n",
    "        inner_batch_size = labels.shape[0]\n",
    "#         print('OUTER: {}, INNER: {}'.format(outer_batch_size, inner_batch_size))\n",
    "\n",
    "        # Prepare same class vector\n",
    "        X_labels = labels.view(-1, 1).repeat(1, outer_batch_size).view(1, inner_batch_size * outer_batch_size)\n",
    "        batch_labels = batch_y.repeat(1, inner_batch_size)\n",
    "        same = (X_labels == batch_labels).to(device, dtype=torch.float32)\n",
    "#         print(same.sum())\n",
    "        same = same * (SESSIONS[SESSION] ** 2) - 1\n",
    "\n",
    "        # Prepare matrices\n",
    "        dist_X = encoded.repeat(1, outer_batch_size).view(-1, ENCODE_DIM)\n",
    "        dist_batch = batch_encoded.expand(inner_batch_size, outer_batch_size, ENCODE_DIM).reshape(-1, ENCODE_DIM)\n",
    "\n",
    "        # Calculate pairwise distances\n",
    "        # TODO: divide by BATCH_SIZE?!\n",
    "        sep = (self.pdist(dist_X, dist_batch) * same).sum() / BATCH_SIZE\n",
    "        if self.training:\n",
    "            writer.add_scalar('data/sep', sep, n_iter)\n",
    " \n",
    "        mse = F.mse_loss(x_pred, x_true)\n",
    "        if self.training:\n",
    "            writer.add_scalar('data/mse', mse, n_iter)\n",
    "#         print('sep: {:.4f}, mse: {:.4f}'.format(sep, mse))\n",
    "        return sep + mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, batch_idx, loss_fn):\n",
    "    loss_fn.train()\n",
    "    losses = np.array([])\n",
    "    for data in train_loader:\n",
    "        vec, labels = data\n",
    "        vec = Variable(vec, requires_grad=True).cpu()\n",
    "        \n",
    "        # ===================forward=====================\n",
    "        dec, enc = model(vec)\n",
    "        loss = distance(dec, vec, enc, labels, batch_idx)\n",
    "        losses = np.append(losses, loss.item())\n",
    "        \n",
    "        # ===================backward====================\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        batch_idx += 1\n",
    "    avg_loss = np.average(losses)\n",
    "    \n",
    "    # ===================validate========================\n",
    "    loss_fn.eval()\n",
    "    val_losses = np.array([])\n",
    "    for data in valid_loader:\n",
    "        vec, labels = data\n",
    "        vec = Variable(vec, requires_grad=False).cpu()\n",
    "        \n",
    "        # ===================forward=====================\n",
    "        dec, enc = model(vec)\n",
    "        val_loss = distance(dec, vec, enc, labels, 0).item()\n",
    "        val_losses = np.append(val_losses, val_loss)\n",
    "    \n",
    "    avg_val_loss = np.average(val_losses)\n",
    "    \n",
    "    # ===================log========================\n",
    "    print('epoch [{}/{}], train_loss: {:.4f}, val_loss: {:.4f}'.format(epoch + 1, EPOCHS, avg_loss, avg_val_loss))\n",
    "    writer.add_scalars('data/loss', {'train_loss': avg_loss,\n",
    "                                     'val_loss': avg_val_loss}, \n",
    "                                     epoch + 1)\n",
    "    \n",
    "    # Add embeddings\n",
    "    if LOG_EMBED:\n",
    "        if epoch % 10 == 0:\n",
    "            encs = torch.Tensor([])\n",
    "            labels = []\n",
    "            for data in train_loader:\n",
    "                vec, label = data\n",
    "                vec = Variable(vec).cpu()\n",
    "                _, enc = model(vec)\n",
    "\n",
    "                encs = torch.cat((encs, enc))\n",
    "                labels.extend([str(l.tolist()) for l in label])\n",
    "            \n",
    "        writer.add_embedding(encs, metadata=labels, global_step=epoch + 1, tag='train')\n",
    "        \n",
    "    # writer.export_scalars_to_json('./all_scalars.json')\n",
    "    return batch_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [1/100], train_loss: 19.8944, val_loss: 0.1980\n",
      "epoch [2/100], train_loss: 0.2026, val_loss: 0.1976\n",
      "epoch [3/100], train_loss: 0.2012, val_loss: 0.1946\n",
      "epoch [4/100], train_loss: 0.1994, val_loss: 0.1919\n",
      "epoch [5/100], train_loss: 0.1972, val_loss: 0.1914\n",
      "epoch [6/100], train_loss: 0.1951, val_loss: 0.1892\n",
      "epoch [7/100], train_loss: 0.1938, val_loss: 0.1860\n",
      "epoch [8/100], train_loss: 0.1915, val_loss: 0.1827\n",
      "epoch [9/100], train_loss: 0.1888, val_loss: 0.1825\n",
      "epoch [10/100], train_loss: 0.1856, val_loss: 0.1812\n",
      "epoch [11/100], train_loss: 0.1835, val_loss: 0.1762\n",
      "epoch [12/100], train_loss: 0.1826, val_loss: 0.1813\n",
      "epoch [13/100], train_loss: 0.1793, val_loss: 0.1756\n",
      "epoch [14/100], train_loss: 0.1784, val_loss: 0.1761\n",
      "epoch [15/100], train_loss: 0.1752, val_loss: 0.1724\n",
      "epoch [16/100], train_loss: 0.1746, val_loss: 0.1718\n",
      "epoch [17/100], train_loss: 0.1731, val_loss: 0.1709\n",
      "epoch [18/100], train_loss: 0.1693, val_loss: 0.1672\n",
      "epoch [19/100], train_loss: 0.1685, val_loss: 0.1666\n",
      "epoch [20/100], train_loss: 0.1677, val_loss: 0.1649\n",
      "epoch [21/100], train_loss: 0.1657, val_loss: 0.1637\n",
      "epoch [22/100], train_loss: 0.1639, val_loss: 0.1625\n",
      "epoch [23/100], train_loss: 0.1625, val_loss: 0.1587\n",
      "epoch [24/100], train_loss: 0.1612, val_loss: 0.1582\n",
      "epoch [25/100], train_loss: 0.1598, val_loss: 0.1544\n",
      "epoch [26/100], train_loss: 0.1574, val_loss: 0.1553\n",
      "epoch [27/100], train_loss: 0.1564, val_loss: 0.1541\n",
      "epoch [28/100], train_loss: 0.1549, val_loss: 0.1544\n",
      "epoch [29/100], train_loss: 0.1524, val_loss: 0.1509\n",
      "epoch [30/100], train_loss: 0.1516, val_loss: 0.1485\n",
      "epoch [31/100], train_loss: 0.1498, val_loss: 0.1491\n",
      "epoch [32/100], train_loss: 0.1484, val_loss: 0.1446\n",
      "epoch [33/100], train_loss: 0.1463, val_loss: 0.1457\n",
      "epoch [34/100], train_loss: 0.1465, val_loss: 0.1436\n",
      "epoch [35/100], train_loss: 0.1452, val_loss: 0.1445\n",
      "epoch [36/100], train_loss: 0.1433, val_loss: 0.1406\n",
      "epoch [37/100], train_loss: 0.1408, val_loss: 0.1397\n",
      "epoch [38/100], train_loss: 0.1397, val_loss: 0.1383\n",
      "epoch [39/100], train_loss: 0.1366, val_loss: 0.1379\n",
      "epoch [40/100], train_loss: 0.1373, val_loss: 0.1373\n",
      "epoch [41/100], train_loss: 0.1346, val_loss: 0.1321\n",
      "epoch [42/100], train_loss: 0.1360, val_loss: 0.1322\n",
      "epoch [43/100], train_loss: 0.1344, val_loss: 0.1324\n",
      "epoch [44/100], train_loss: 0.1329, val_loss: 0.1315\n",
      "epoch [45/100], train_loss: 0.1311, val_loss: 0.1269\n",
      "epoch [46/100], train_loss: 0.1301, val_loss: 0.1271\n",
      "epoch [47/100], train_loss: 0.1290, val_loss: 0.1304\n",
      "epoch [48/100], train_loss: 0.1278, val_loss: 0.1252\n",
      "epoch [49/100], train_loss: 0.1258, val_loss: 0.1242\n",
      "epoch [50/100], train_loss: 0.1247, val_loss: 0.1255\n",
      "epoch [51/100], train_loss: 0.1250, val_loss: 0.1198\n",
      "epoch [52/100], train_loss: 0.1241, val_loss: 0.1224\n",
      "epoch [53/100], train_loss: 0.1221, val_loss: 0.1175\n",
      "epoch [54/100], train_loss: 0.1219, val_loss: 0.1181\n",
      "epoch [55/100], train_loss: 0.1209, val_loss: 0.1181\n",
      "epoch [56/100], train_loss: 0.1177, val_loss: 0.1187\n",
      "epoch [57/100], train_loss: 0.1180, val_loss: 0.1152\n",
      "epoch [58/100], train_loss: 0.1187, val_loss: 0.1109\n",
      "epoch [59/100], train_loss: 0.1154, val_loss: 0.1098\n",
      "epoch [60/100], train_loss: 0.1158, val_loss: 0.1115\n",
      "epoch [61/100], train_loss: 0.1156, val_loss: 0.1088\n",
      "epoch [62/100], train_loss: 0.1122, val_loss: 0.1080\n",
      "epoch [63/100], train_loss: 0.1118, val_loss: 0.1077\n",
      "epoch [64/100], train_loss: 0.1128, val_loss: 0.1062\n",
      "epoch [65/100], train_loss: 0.1111, val_loss: 0.1057\n",
      "epoch [66/100], train_loss: 0.1103, val_loss: 0.1051\n",
      "epoch [67/100], train_loss: 0.1081, val_loss: 0.1015\n",
      "epoch [68/100], train_loss: 0.1081, val_loss: 0.1014\n",
      "epoch [69/100], train_loss: 0.1072, val_loss: 0.1050\n",
      "epoch [70/100], train_loss: 0.1055, val_loss: 0.1054\n",
      "epoch [71/100], train_loss: 0.1035, val_loss: 0.1039\n",
      "epoch [72/100], train_loss: 0.1040, val_loss: 0.0997\n",
      "epoch [73/100], train_loss: 0.1023, val_loss: 0.1008\n",
      "epoch [74/100], train_loss: 0.1022, val_loss: 0.0991\n",
      "epoch [75/100], train_loss: 0.1020, val_loss: 0.0974\n",
      "epoch [76/100], train_loss: 0.0997, val_loss: 0.1007\n",
      "epoch [77/100], train_loss: 0.0992, val_loss: 0.0994\n",
      "epoch [78/100], train_loss: 0.0995, val_loss: 0.0966\n",
      "epoch [79/100], train_loss: 0.0990, val_loss: 0.0973\n",
      "epoch [80/100], train_loss: 0.0977, val_loss: 0.0971\n",
      "epoch [81/100], train_loss: 0.0963, val_loss: 0.0935\n",
      "epoch [82/100], train_loss: 0.0950, val_loss: 0.0943\n",
      "epoch [83/100], train_loss: 0.0943, val_loss: 0.0954\n",
      "epoch [84/100], train_loss: 0.0956, val_loss: 0.0928\n",
      "epoch [85/100], train_loss: 0.0932, val_loss: 0.0918\n",
      "epoch [86/100], train_loss: 0.0924, val_loss: 0.0914\n",
      "epoch [87/100], train_loss: 0.0921, val_loss: 0.0924\n",
      "epoch [88/100], train_loss: 0.0925, val_loss: 0.0925\n",
      "epoch [89/100], train_loss: 0.0926, val_loss: 0.0873\n",
      "epoch [90/100], train_loss: 0.0909, val_loss: 0.0889\n",
      "epoch [91/100], train_loss: 0.0910, val_loss: 0.0867\n",
      "epoch [92/100], train_loss: 0.0903, val_loss: 0.0889\n",
      "epoch [93/100], train_loss: 0.0894, val_loss: 0.0877\n",
      "epoch [94/100], train_loss: 0.0890, val_loss: 0.0877\n",
      "epoch [95/100], train_loss: 0.0891, val_loss: 0.0868\n",
      "epoch [96/100], train_loss: 0.0887, val_loss: 0.0863\n",
      "epoch [97/100], train_loss: 0.0880, val_loss: 0.0826\n",
      "epoch [98/100], train_loss: 0.0876, val_loss: 0.0830\n",
      "epoch [99/100], train_loss: 0.0858, val_loss: 0.0833\n",
      "epoch [100/100], train_loss: 0.0857, val_loss: 0.0850\n"
     ]
    }
   ],
   "source": [
    "# writer = SummaryWriter(log_dir='runs/SeparatorTestEmbed')\n",
    "writer = SummaryWriter()\n",
    "model = Autoencoder().cpu()\n",
    "distance = SeparatorLoss(lossloader, model.encoder)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.0003, momentum = 0.9)\n",
    "\n",
    "train_idx = 0\n",
    "for epoch in range(EPOCHS):\n",
    "    train_idx = train(epoch, train_idx, distance)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.43354136 0.4235023  0.42541125 0.42280942 0.40392163 0.43220171\n",
      " 0.42916408 0.43900755 0.25190908]\n",
      "test_loss: 0.4068\n"
     ]
    }
   ],
   "source": [
    "distance = SeparatorLoss(testlossloader, model.encoder)\n",
    "distance.eval()\n",
    "losses = np.array([])\n",
    "for data in test_loader:\n",
    "    vec, labels = data\n",
    "    vec = Variable(vec, requires_grad=False).cpu()\n",
    "\n",
    "    # ===================forward=====================\n",
    "    dec, enc = model(vec)\n",
    "    loss = distance(dec, vec, enc, labels, 0)\n",
    "    losses = np.append(losses, loss.item())\n",
    "print(losses)\n",
    "avg_loss = np.average(losses)\n",
    "\n",
    "print('test_loss: {:.4f}'.format(avg_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
